{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydicom\n",
    "import nibabel as nib\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from time import perf_counter\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDcmPet(pathToDcmFolder):\n",
    "\n",
    "    locs=[]\n",
    "    for i in range(len(os.listdir(pathToDcmFolder))):\n",
    "        file='{}/{}'.format(pathToDcmFolder,os.listdir(pathToDcmFolder)[i])\n",
    "        ds=pydicom.dcmread(file,force=True)\n",
    "        if (0x0020,0x1041) in ds:\n",
    "            locs.append(ds.SliceLocation)\n",
    "            i0=ds.pixel_array.shape[0]\n",
    "            i1=ds.pixel_array.shape[1]\n",
    "    uniLocs=np.sort(np.unique(np.array(locs)))\n",
    "    img3d=np.zeros((i0,i1,len(uniLocs)))\n",
    "    for i in range(len(os.listdir(pathToDcmFolder))):\n",
    "        file='{}/{}'.format(pathToDcmFolder,os.listdir(pathToDcmFolder)[i])\n",
    "        ds=pydicom.dcmread(file,force=True)\n",
    "        if (0x0020,0x1041) in ds:\n",
    "            for j in range(len(uniLocs)):\n",
    "                if uniLocs[j]==ds.SliceLocation:\n",
    "                    if (0x0028, 0x1053) in ds and (0x0028, 0x1052) in ds:\n",
    "                        img3d[:,:,j]=ds.pixel_array*ds[0x0028, 0x1053].value+ds[0x0028, 0x1052].value\n",
    "                    else:\n",
    "                        img3d[:,:,j]=ds.pixel_array\n",
    "    return(img3d)\n",
    "\n",
    "\n",
    "def preProcess(img,img_height):\n",
    "\n",
    "    img=cv2.resize(img,(img_height,img_height))\n",
    "    img=(img-np.min(img))/(np.max(img)-np.min(img))\n",
    "    return(img.astype('float32'))\n",
    "\n",
    "def loadImgSlices(img_height):\n",
    "\n",
    "    df1=pd.read_excel('D:/rpe/DL-RPE-potilaat.xlsx')\n",
    "    negSlices=[]\n",
    "    rpeSlices=[]\n",
    "    rpeIndexes=[]\n",
    "    caseNames0=[]\n",
    "    caseNames1=[]\n",
    "    for i in range(len(df1['caseID'])):\n",
    "        pathToDcmFolder='D:/rpe/anon_dixon/{}'.format(df1['caseID'][i])\n",
    "        img3d=readDcmPet(pathToDcmFolder)\n",
    "        slices=[]\n",
    "        for j in range(img3d.shape[2]):\n",
    "            slices.append(preProcess(img3d[:,:,j],img_height))\n",
    "        if df1['rpe'][i]==0:\n",
    "            negSlices.append(slices)\n",
    "            caseNames0.append(df1['caseID'][i])\n",
    "        else:\n",
    "            rpeSlices.append(slices)\n",
    "            indexes=np.zeros((img3d.shape[2]))\n",
    "            indexes[int(df1['start_slice'][i]):(int(df1['end_slice'][i])+1)]=1\n",
    "            rpeIndexes.append(indexes)\n",
    "            caseNames1.append(df1['caseID'][i])\n",
    "    return(negSlices,rpeSlices,rpeIndexes,caseNames0,caseNames1)\n",
    "\n",
    "def divideIntoSets(negSlices,rpeSlices,rpeIndexes,caseNames0,caseNames1,k):\n",
    "\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    x_test=[]\n",
    "    y_test=[]\n",
    "    trainIndexes=[]\n",
    "    testIndexes=[]\n",
    "    testCases=[]\n",
    "\n",
    "    for i in range(len(rpeSlices)):\n",
    "        if i%5==k:\n",
    "            for j in range(len(rpeSlices[i])):\n",
    "                x_test.append(rpeSlices[i][j])\n",
    "                y_test.append(rpeIndexes[i][j])\n",
    "                testIndexes.append(j)\n",
    "            testCases.append(caseNames1[i])\n",
    "            if i<len(negSlices):\n",
    "                for j in range(len(negSlices[i])):\n",
    "                    x_test.append(negSlices[i][j])\n",
    "                    y_test.append(0)\n",
    "                    testIndexes.append(j) \n",
    "                testCases.append(caseNames0[i])   \n",
    "        else:\n",
    "            for j in range(len(rpeSlices[i])):\n",
    "                x_train.append(rpeSlices[i][j])\n",
    "                y_train.append(rpeIndexes[i][j])\n",
    "                trainIndexes.append(j)\n",
    "            if i<len(negSlices):\n",
    "                for j in range(len(negSlices[i])):\n",
    "                    x_train.append(negSlices[i][j])\n",
    "                    y_train.append(0)  \n",
    "                    trainIndexes.append(j)\n",
    "\n",
    "    x_train=np.array(x_train)\n",
    "    y_train=np.array(y_train)    \n",
    "    x_test=np.array(x_test)\n",
    "    y_test=np.array(y_test)\n",
    "    trainIndexes=np.array(trainIndexes)\n",
    "    testIndexes=np.array(testIndexes)\n",
    "    return(x_train,y_train,x_test,y_test,trainIndexes,testIndexes,testCases)\n",
    "\n",
    "def augment(x_train,y_train,type):\n",
    "\n",
    "    x_train1=[]\n",
    "    y_train1=[]\n",
    "    for i in range(x_train.shape[0]):\n",
    "        x_train1.append(x_train[i])\n",
    "        y_train1.append(y_train[i])\n",
    "    for i in range(x_train.shape[0]):\n",
    "        if type=='flip':\n",
    "            x_train1.append(cv2.flip(x_train[i],1))\n",
    "        if type=='rotate90':\n",
    "            x_train1.append(cv2.rotate(x_train[i],cv2.ROTATE_90_CLOCKWISE))\n",
    "        if type=='blur':\n",
    "            x_train1.append(cv2.filter2D(x_train[i],-1,np.ones((3,3))/25))\n",
    "        y_train1.append(y_train[i])\n",
    "    x_train1=np.array(x_train1)\n",
    "    y_train1=np.array(y_train1)\n",
    "    return(x_train1,y_train1)\n",
    "\n",
    "def predict(x_train,y_train,x_test,numberOfEpochs):\n",
    "    \n",
    "    img_height=x_train[0].shape[0]\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Conv2D(16, 3, activation='relu', input_shape=(img_height,img_height,1)),\n",
    "                        keras.layers.Conv2D(16, 3, activation='relu'),\n",
    "                        keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "                        keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "                        keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "                        keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "                        keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "                        keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "                        keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "                        keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "                        keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "                        keras.layers.MaxPooling2D(strides=(2, 2)),\n",
    "                        keras.layers.Flatten(),\n",
    "                        keras.layers.Dense(128, activation='relu'),\n",
    "                        keras.layers.Dense(64, activation='relu'),\n",
    "                        keras.layers.Dense(32, activation='relu'),\n",
    "                        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "        mode='min', patience=5, restore_best_weights=True)\n",
    "\n",
    "    history=model.fit(x=x_train,y=y_train,epochs=numberOfEpochs,validation_split=0.3,callbacks=[earlystopping],shuffle=True)\n",
    "    predictions=model.predict(x_test)\n",
    "    trainPreds=model.predict(x_train)\n",
    "    plt.plot(range(len(history.history['loss'])),history.history['loss'],color='blue')\n",
    "\n",
    "    return([predictions,trainPreds])\n",
    "\n",
    "\n",
    "def predictInceptionV3(x_train,y_train,x_test,numberOfEpochs):\n",
    "\n",
    "    img_height=x_train[0].shape[0]\n",
    "\n",
    "    x_train1=[]\n",
    "    for i in range(x_train.shape[0]):\n",
    "        x_train1.append(np.rollaxis(np.array([x_train[i],x_train[i],x_train[i]]),0,3))\n",
    "    x_train1=np.array(x_train1)\n",
    "    \n",
    "    x_test1=[]\n",
    "    for i in range(x_test.shape[0]):\n",
    "        x_test1.append(np.rollaxis(np.array([x_test[i],x_test[i],x_test[i]]),0,3))\n",
    "    x_test1=np.array(x_test1)\n",
    "\n",
    "    ntf_model = keras.applications.InceptionV3(weights=None,input_shape=(img_height,img_height,3),include_top=False)\n",
    "\n",
    "    ntf_model.trainable = True\n",
    "    inputs = keras.Input(shape=(img_height,img_height,3))\n",
    "    x = ntf_model(inputs, training=True)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=[keras.metrics.BinaryAccuracy()])\n",
    "    \n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "        mode='min', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    history=model.fit(x=x_train1, y=y_train, epochs=numberOfEpochs, validation_split=0.3, callbacks=[earlystopping], shuffle=True)\n",
    "    predictions=model.predict(x_test1)\n",
    "    trainPreds=model.predict(x_train1)\n",
    "    plt.plot(range(len(history.history['loss'])),history.history['loss'],color='blue')\n",
    "\n",
    "    return([predictions,trainPreds])\n",
    "\n",
    "def evaluatePreds(predictions,y_test,trainPreds,y_train):\n",
    "\n",
    "    default=0.5\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, trainPreds, drop_intermediate=False)\n",
    "    J_stats = tpr - fpr\n",
    "    youden = thresholds[np.argmax(J_stats)]\n",
    "    rocdists = (1-tpr)**2+fpr**2\n",
    "    minRocDist = thresholds[np.argmin(rocdists)]\n",
    "    equ = thresholds[np.argmin((tpr-1+fpr)**2)]\n",
    "    threshold=youden\n",
    "\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        if predictions[i]<threshold:\n",
    "            if y_test[i]==0:\n",
    "                TN+=1\n",
    "            else:\n",
    "                FN+=1\n",
    "        else:\n",
    "            if y_test[i]==1:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "\n",
    "    acc = (TN+TP)/(TN+FN+TP+FP)\n",
    "    sen = TP/(FN+TP)\n",
    "    spe = TN/(TN+FP)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions, drop_intermediate=False)\n",
    "    auc1=auc(fpr, tpr)\n",
    "\n",
    "    return([acc,sen,spe,auc1])\n",
    "\n",
    "def convertIntoVectors(y_train,y_test,trainIndexes,testIndexes,trainPreds,predictions):\n",
    "\n",
    "    x_train_vectors=[]\n",
    "    y_train_vectors=[]\n",
    "    x_test_vectors=[]\n",
    "    y_test_vectors=[]\n",
    "\n",
    "    vector=[]\n",
    "    y_vector=[]\n",
    "    for i in range(len(trainIndexes)):\n",
    "        vector.append(trainPreds[i])\n",
    "        y_vector.append(y_train[i])\n",
    "        if i==len(trainIndexes)-1:\n",
    "            x_train_vectors.append(vector)\n",
    "            y_train_vectors.append(y_vector)\n",
    "        else:\n",
    "            if trainIndexes[i+1]==0:\n",
    "                x_train_vectors.append(vector)\n",
    "                y_train_vectors.append(y_vector)\n",
    "                vector=[]\n",
    "                y_vector=[]\n",
    "\n",
    "    vector=[]\n",
    "    y_vector=[]\n",
    "    for i in range(len(testIndexes)):\n",
    "        vector.append(predictions[i])\n",
    "        y_vector.append(y_test[i])\n",
    "        if i==len(testIndexes)-1:\n",
    "            x_test_vectors.append(vector)\n",
    "            y_test_vectors.append(y_vector)\n",
    "        else:\n",
    "            if testIndexes[i+1]==0:\n",
    "                x_test_vectors.append(vector)\n",
    "                y_test_vectors.append(y_vector)\n",
    "                vector=[]\n",
    "                y_vector=[]\n",
    "\n",
    "    return(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors)\n",
    "\n",
    "def evaluateVectors(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors):\n",
    "\n",
    "    thresholds=[]\n",
    "    indexes=[]\n",
    "    for i in range(len(x_train_vectors)):\n",
    "        means=[]\n",
    "        for j in range(len(x_train_vectors[i])-4):\n",
    "            means.append(np.mean([x_train_vectors[i][j],x_train_vectors[i][j+1],x_train_vectors[i][j+2],x_train_vectors[i][j+3],x_train_vectors[i][j+4]]))\n",
    "        thresholds.append(max(means))\n",
    "        indexes.append(max(y_train_vectors[i]))\n",
    "    thresholds=np.array(thresholds)\n",
    "    indexes=np.array(indexes)\n",
    "    thresholds0=thresholds[indexes==0]\n",
    "    thresholds1=thresholds[indexes==1]\n",
    "\n",
    "    thresholds=np.unique(thresholds)\n",
    "    fpr=[]\n",
    "    tpr=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        tpr.append((thresholds1[thresholds1>=thresholds[i]]).shape[0]/thresholds1.shape[0])\n",
    "        fpr.append((thresholds0[thresholds0>=thresholds[i]]).shape[0]/thresholds0.shape[0])\n",
    "    tpr=np.array(tpr)\n",
    "    fpr=np.array(fpr)\n",
    "    J_stats = tpr - fpr\n",
    "    youden = thresholds[np.argmax(J_stats)]\n",
    "\n",
    "    predictions1=[]\n",
    "    indexes1=[]\n",
    "    for i in range(len(x_train_vectors)):\n",
    "        means=[]\n",
    "        for j in range(len(x_train_vectors[i])-4):\n",
    "            means.append(np.mean([x_train_vectors[i][j],x_train_vectors[i][j+1],x_train_vectors[i][j+2],x_train_vectors[i][j+3],x_train_vectors[i][j+4]]))\n",
    "        if max(means)>=youden:\n",
    "            predictions1+=x_train_vectors[i]\n",
    "            indexes1+=y_train_vectors[i]\n",
    "    predictions1=np.array(predictions1)\n",
    "    indexes1=np.array(indexes1)\n",
    "    fpr, tpr, thresholds = roc_curve(indexes1, predictions1, drop_intermediate=False)\n",
    "    J_stats = tpr - fpr\n",
    "    youden1 = thresholds[np.argmax(J_stats)]\n",
    "\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "\n",
    "    TN_1 = 0\n",
    "    FN_1 = 0\n",
    "    TP_1 = 0\n",
    "    FP_1 = 0\n",
    "\n",
    "    thresholds=[]\n",
    "    indexes=[]\n",
    "    predictions1=[]\n",
    "    indexes1=[]\n",
    "    for i in range(len(x_test_vectors)):\n",
    "        means=[]\n",
    "        for j in range(len(x_test_vectors[i])-4):\n",
    "            means.append(np.mean([x_test_vectors[i][j],x_test_vectors[i][j+1],x_test_vectors[i][j+2],x_test_vectors[i][j+3],x_test_vectors[i][j+4]]))\n",
    "        thresholds.append(max(means))\n",
    "        indexes.append(max(y_test_vectors[i]))\n",
    "        if max(means)<youden:\n",
    "            if indexes[i]==0:\n",
    "                TN+=1\n",
    "                TN_1+=len(y_test_vectors[i])\n",
    "            else:\n",
    "                FN+=1\n",
    "                FN_1+=np.sum(y_test_vectors[i])\n",
    "                TN_1+=len(y_test_vectors[i])-np.sum(y_test_vectors[i])\n",
    "            predictions1+=list(np.zeros(len(y_test_vectors[i])))\n",
    "            indexes1+=y_test_vectors[i]\n",
    "        else:\n",
    "            if indexes[i]==1:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "            for j in range(len(x_test_vectors[i])):\n",
    "                if x_test_vectors[i][j]<youden1:\n",
    "                    if y_test_vectors[i][j]==0:\n",
    "                        TN_1+=1\n",
    "                    else:\n",
    "                        FN_1+=1\n",
    "                else:\n",
    "                    if y_test_vectors[i][j]==1:\n",
    "                        TP_1+=1\n",
    "                    else:\n",
    "                        FP_1+=1\n",
    "            predictions1+=x_test_vectors[i]\n",
    "            indexes1+=y_test_vectors[i]\n",
    "    thresholds=np.array(thresholds)\n",
    "    indexes=np.array(indexes)\n",
    "    thresholds0=thresholds[indexes==0]\n",
    "    thresholds1=thresholds[indexes==1]\n",
    "    thresholds=np.unique(thresholds)\n",
    "    fpr=[]\n",
    "    tpr=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        tpr.append((thresholds1[thresholds1>=thresholds[i]]).shape[0]/thresholds1.shape[0])\n",
    "        fpr.append((thresholds0[thresholds0>=thresholds[i]]).shape[0]/thresholds0.shape[0])\n",
    "    tpr=np.array(tpr)\n",
    "    fpr=np.array(fpr)\n",
    "    auc1_1=auc(fpr, tpr)\n",
    "    predictions1=np.array(predictions1)\n",
    "    indexes1=np.array(indexes1)\n",
    "    fpr, tpr, thresholds = roc_curve(indexes1, predictions1, drop_intermediate=False)\n",
    "    auc1=auc(fpr, tpr)\n",
    "\n",
    "    acc = (TN+TP)/(TN+FN+TP+FP)\n",
    "    sen = TP/(FN+TP)\n",
    "    spe = TN/(TN+FP)\n",
    "    acc_1 = (TN_1+TP_1)/(TN_1+FN_1+TP_1+FP_1)\n",
    "    sen_1 = TP_1/(FN_1+TP_1)\n",
    "    spe_1 = TN_1/(TN_1+FP_1)\n",
    "    return([acc,sen,spe,auc1,acc_1,sen_1,spe_1,auc1_1])\n",
    "\n",
    "def listFinalIndexes(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors,testCases):\n",
    "\n",
    "    thresholds=[]\n",
    "    indexes=[]\n",
    "    for i in range(len(x_train_vectors)):\n",
    "        means=[]\n",
    "        for j in range(len(x_train_vectors[i])-4):\n",
    "            means.append(np.mean([x_train_vectors[i][j],x_train_vectors[i][j+1],x_train_vectors[i][j+2],x_train_vectors[i][j+3],x_train_vectors[i][j+4]]))\n",
    "        thresholds.append(max(means))\n",
    "        indexes.append(max(y_train_vectors[i]))\n",
    "    thresholds=np.array(thresholds)\n",
    "    indexes=np.array(indexes)\n",
    "    thresholds0=thresholds[indexes==0]\n",
    "    thresholds1=thresholds[indexes==1]\n",
    "\n",
    "    thresholds=np.unique(thresholds)\n",
    "    fpr=[]\n",
    "    tpr=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        tpr.append((thresholds1[thresholds1>=thresholds[i]]).shape[0]/thresholds1.shape[0])\n",
    "        fpr.append((thresholds0[thresholds0>=thresholds[i]]).shape[0]/thresholds0.shape[0])\n",
    "    tpr=np.array(tpr)\n",
    "    fpr=np.array(fpr)\n",
    "    J_stats = tpr - fpr\n",
    "    youden = thresholds[np.argmax(J_stats)]\n",
    "\n",
    "    predictions1=[]\n",
    "    indexes1=[]\n",
    "    for i in range(len(x_train_vectors)):\n",
    "        means=[]\n",
    "        for j in range(len(x_train_vectors[i])-4):\n",
    "            means.append(np.mean([x_train_vectors[i][j],x_train_vectors[i][j+1],x_train_vectors[i][j+2],x_train_vectors[i][j+3],x_train_vectors[i][j+4]]))\n",
    "        if max(means)>=youden:\n",
    "            predictions1+=x_train_vectors[i]\n",
    "            indexes1+=y_train_vectors[i]\n",
    "    predictions1=np.array(predictions1)\n",
    "    indexes1=np.array(indexes1)\n",
    "    fpr, tpr, thresholds = roc_curve(indexes1, predictions1, drop_intermediate=False)\n",
    "    J_stats = tpr - fpr\n",
    "    youden1 = thresholds[np.argmax(J_stats)]\n",
    "\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "\n",
    "    TN_1 = 0\n",
    "    FN_1 = 0\n",
    "    TP_1 = 0\n",
    "    FP_1 = 0\n",
    "\n",
    "    thresholds=[]\n",
    "    indexes=[]\n",
    "    predictions1=[]\n",
    "    indexes1=[]\n",
    "    TN_indexes=[]\n",
    "    TP_indexes=[]\n",
    "    FN_indexes=[]\n",
    "    FP_indexes=[]\n",
    "    FN_patients=[]\n",
    "    FP_patients=[]\n",
    "    for i in range(len(x_test_vectors)):\n",
    "        means=[]\n",
    "        for j in range(len(x_test_vectors[i])-4):\n",
    "            means.append(np.mean([x_test_vectors[i][j],x_test_vectors[i][j+1],x_test_vectors[i][j+2],x_test_vectors[i][j+3],x_test_vectors[i][j+4]]))\n",
    "        thresholds.append(max(means))\n",
    "        indexes.append(max(y_test_vectors[i]))\n",
    "        if max(means)<youden:\n",
    "            if indexes[i]==0:\n",
    "                TN+=1\n",
    "                TN_1+=len(y_test_vectors[i])\n",
    "                TN_indexes.append('{}_all'.format(i))\n",
    "            else:\n",
    "                FN+=1\n",
    "                FN_1+=np.sum(y_test_vectors[i])\n",
    "                TN_1+=len(y_test_vectors[i])-np.sum(y_test_vectors[i])\n",
    "                FN_indexes.append('{}_allP'.format(i))\n",
    "                TN_indexes.append('{}_allN'.format(i))\n",
    "                FN_patients.append(testCases[i])\n",
    "            predictions1+=list(np.zeros(len(y_test_vectors[i])))\n",
    "            indexes1+=y_test_vectors[i]\n",
    "        else:\n",
    "            if indexes[i]==1:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "                FP_patients.append(testCases[i])\n",
    "            for j in range(len(x_test_vectors[i])):\n",
    "                if x_test_vectors[i][j]<youden1:\n",
    "                    if y_test_vectors[i][j]==0:\n",
    "                        TN_1+=1\n",
    "                        TN_indexes.append('{}_{}'.format(i,j))\n",
    "                    else:\n",
    "                        FN_1+=1\n",
    "                        FN_indexes.append('{}_{}'.format(i,j))\n",
    "                else:\n",
    "                    if y_test_vectors[i][j]==1:\n",
    "                        TP_1+=1\n",
    "                        TP_indexes.append('{}_{}'.format(i,j))\n",
    "                    else:\n",
    "                        FP_1+=1\n",
    "                        FP_indexes.append('{}_{}'.format(i,j))\n",
    "            predictions1+=x_test_vectors[i]\n",
    "            indexes1+=y_test_vectors[i]\n",
    "\n",
    "    return(TN_indexes,TP_indexes,FN_indexes,FP_indexes,FN_patients,FP_patients)\n",
    "\n",
    "def infoForROC(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors):\n",
    "\n",
    "    thresholds=[]\n",
    "    indexes=[]\n",
    "    for i in range(len(x_train_vectors)):\n",
    "        means=[]\n",
    "        for j in range(len(x_train_vectors[i])-4):\n",
    "            means.append(np.mean([x_train_vectors[i][j],x_train_vectors[i][j+1],x_train_vectors[i][j+2],x_train_vectors[i][j+3],x_train_vectors[i][j+4]]))\n",
    "        thresholds.append(max(means))\n",
    "        indexes.append(max(y_train_vectors[i]))\n",
    "    thresholds=np.array(thresholds)\n",
    "    indexes=np.array(indexes)\n",
    "    thresholds0=thresholds[indexes==0]\n",
    "    thresholds1=thresholds[indexes==1]\n",
    "\n",
    "    thresholds=np.unique(thresholds)\n",
    "    fpr=[]\n",
    "    tpr=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        tpr.append((thresholds1[thresholds1>=thresholds[i]]).shape[0]/thresholds1.shape[0])\n",
    "        fpr.append((thresholds0[thresholds0>=thresholds[i]]).shape[0]/thresholds0.shape[0])\n",
    "    tpr=np.array(tpr)\n",
    "    fpr=np.array(fpr)\n",
    "    J_stats = tpr - fpr\n",
    "    youden = thresholds[np.argmax(J_stats)]\n",
    "\n",
    "    predictions1=[]\n",
    "    indexes1=[]\n",
    "    for i in range(len(x_train_vectors)):\n",
    "        means=[]\n",
    "        for j in range(len(x_train_vectors[i])-4):\n",
    "            means.append(np.mean([x_train_vectors[i][j],x_train_vectors[i][j+1],x_train_vectors[i][j+2],x_train_vectors[i][j+3],x_train_vectors[i][j+4]]))\n",
    "        if max(means)>=youden:\n",
    "            predictions1+=x_train_vectors[i]\n",
    "            indexes1+=y_train_vectors[i]\n",
    "    predictions1=np.array(predictions1)\n",
    "    indexes1=np.array(indexes1)\n",
    "    fpr, tpr, thresholds = roc_curve(indexes1, predictions1, drop_intermediate=False)\n",
    "    J_stats = tpr - fpr\n",
    "    youden1 = thresholds[np.argmax(J_stats)]\n",
    "\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "\n",
    "    TN_1 = 0\n",
    "    FN_1 = 0\n",
    "    TP_1 = 0\n",
    "    FP_1 = 0\n",
    "\n",
    "    thresholds=[]\n",
    "    indexes=[]\n",
    "    predictions1=[]\n",
    "    indexes1=[]\n",
    "    for i in range(len(x_test_vectors)):\n",
    "        means=[]\n",
    "        for j in range(len(x_test_vectors[i])-4):\n",
    "            means.append(np.mean([x_test_vectors[i][j],x_test_vectors[i][j+1],x_test_vectors[i][j+2],x_test_vectors[i][j+3],x_test_vectors[i][j+4]]))\n",
    "        thresholds.append(max(means))\n",
    "        indexes.append(max(y_test_vectors[i]))\n",
    "        if max(means)<youden:\n",
    "            if indexes[i]==0:\n",
    "                TN+=1\n",
    "                TN_1+=len(y_test_vectors[i])\n",
    "            else:\n",
    "                FN+=1\n",
    "                FN_1+=np.sum(y_test_vectors[i])\n",
    "                TN_1+=len(y_test_vectors[i])-np.sum(y_test_vectors[i])\n",
    "            predictions1+=list(np.zeros(len(y_test_vectors[i])))\n",
    "            indexes1+=y_test_vectors[i]\n",
    "        else:\n",
    "            if indexes[i]==1:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "            for j in range(len(x_test_vectors[i])):\n",
    "                if x_test_vectors[i][j]<youden1:\n",
    "                    if y_test_vectors[i][j]==0:\n",
    "                        TN_1+=1\n",
    "                    else:\n",
    "                        FN_1+=1\n",
    "                else:\n",
    "                    if y_test_vectors[i][j]==1:\n",
    "                        TP_1+=1\n",
    "                    else:\n",
    "                        FP_1+=1\n",
    "            predictions1+=x_test_vectors[i]\n",
    "            indexes1+=y_test_vectors[i]\n",
    "    thresholds=np.array(thresholds)\n",
    "    indexes=np.array(indexes)\n",
    "    thresholds0=thresholds[indexes==0]\n",
    "    thresholds1=thresholds[indexes==1]\n",
    "    thresholds=np.unique(thresholds)\n",
    "    fpr=[]\n",
    "    tpr=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        tpr.append((thresholds1[thresholds1>=thresholds[i]]).shape[0]/thresholds1.shape[0])\n",
    "        fpr.append((thresholds0[thresholds0>=thresholds[i]]).shape[0]/thresholds0.shape[0])\n",
    "    tpr1=np.array(tpr)\n",
    "    fpr1=np.array(fpr)\n",
    "    predictions1=np.array(predictions1)\n",
    "    indexes1=np.array(indexes1)\n",
    "    fpr, tpr, thresholds = roc_curve(indexes1, predictions1, drop_intermediate=False)\n",
    "\n",
    "    return([tpr,fpr,tpr1,fpr1])\n",
    "\n",
    "def intersection(lst1,lst2):\n",
    "    lst3=[value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def vectorsToSameDimension(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors):\n",
    "\n",
    "    train_real_lengths=[]\n",
    "    test_real_lengths=[]\n",
    "    for i in range(len(x_train_vectors)):\n",
    "        train_real_lengths.append(len(x_train_vectors[i]))\n",
    "    for i in range(len(x_test_vectors)):\n",
    "        test_real_lengths.append(len(x_test_vectors[i]))\n",
    "\n",
    "    maxLength=max(max(train_real_lengths),max(test_real_lengths))\n",
    "    for i in range(len(x_train_vectors)):\n",
    "        if len(x_train_vectors[i])<maxLength:\n",
    "            x_train_vectors[i]=x_train_vectors[i]+list(np.zeros((int(maxLength-len(x_train_vectors[i])))))\n",
    "            y_train_vectors[i]=y_train_vectors[i]+list(np.zeros((int(maxLength-len(y_train_vectors[i])))))\n",
    "    for i in range(len(x_test_vectors)):\n",
    "        if len(x_test_vectors[i])<maxLength:\n",
    "            x_test_vectors[i]=x_test_vectors[i]+list(np.zeros((int(maxLength-len(x_test_vectors[i])))))\n",
    "            y_test_vectors[i]=y_test_vectors[i]+list(np.zeros((int(maxLength-len(y_test_vectors[i])))))\n",
    "\n",
    "    x_train_vectors=np.array(x_train_vectors)\n",
    "    y_train_vectors=np.array(y_train_vectors)\n",
    "    x_test_vectors=np.array(x_test_vectors)\n",
    "    y_test_vectors=np.array(y_test_vectors)\n",
    "    train_real_lengths=np.array(train_real_lengths)\n",
    "    test_real_lengths=np.array(test_real_lengths)\n",
    "\n",
    "    return(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors,train_real_lengths,test_real_lengths)\n",
    "\n",
    "def evaluateLSTMvectors(predicted_vectors,y_test,test_real_lengths,trainPred_vectors,y_train,train_real_lengths):\n",
    "\n",
    "    predictions=[]\n",
    "    trainPreds=[]\n",
    "    for i in range(len(predicted_vectors)):\n",
    "        predictions=predictions+list(predicted_vectors[i][0:test_real_lengths[i]])\n",
    "    for i in range(len(trainPred_vectors)):\n",
    "        trainPreds=trainPreds+list(trainPred_vectors[i][0:train_real_lengths[i]])\n",
    "    predictions=np.array(predictions)\n",
    "    trainPreds=np.array(trainPreds)\n",
    "    y_test=y_test[0:len(predictions)]\n",
    "    y_train=y_train[0:len(trainPreds)]\n",
    "    acc,sen,spe,auc1=evaluatePreds(predictions,y_test,trainPreds,y_train)\n",
    "    return([acc,sen,spe,auc1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToDcmFolder='D:/rpe/anon_dixon/case0001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3d=readDcmPet(pathToDcmFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.filter2D(cv2.resize(img3d[:,:,0],(128,128)),-1,np.ones((3,3))/25),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel('D:/rpe/DL-RPE-potilaat.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height=128\n",
    "negSlices,rpeSlices,rpeIndexes,caseNames0,caseNames1=loadImgSlices(img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(negSlices))\n",
    "print(len(rpeSlices))\n",
    "print(len(rpeIndexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=[]\n",
    "for i in range(len(rpeIndexes)):\n",
    "    j.append(len(rpeIndexes[i]))\n",
    "for i in range(len(negSlices)):\n",
    "    j.append(len(negSlices[i]))\n",
    "print(max(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "x_train,y_train,x_test,y_test,trainIndexes,testIndexes,testCases=divideIntoSets(negSlices,rpeSlices,rpeIndexes,caseNames0,caseNames1,k)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(np.sum(y_train))\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(np.sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1,y_train1=augment(x_train,y_train,'flip')\n",
    "print(x_train1.shape)\n",
    "print(y_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel('D:/rpe/DL-RPE-potilaat.xlsx')\n",
    "j=[]\n",
    "j1=[]\n",
    "j2=[]\n",
    "j3=[]\n",
    "for i in range(len(df1['caseID'])):\n",
    "    pathToDcmFolder='D:/rpe/anon_dixon/{}'.format(df1['caseID'][i])\n",
    "    file='{}/{}'.format(pathToDcmFolder,os.listdir(pathToDcmFolder)[0])\n",
    "    ds=pydicom.dcmread(file,force=True)\n",
    "    j.append(ds.SpacingBetweenSlices)\n",
    "    j1.append(ds.PixelSpacing[0])\n",
    "    j2.append(ds.pixel_array.shape[0])\n",
    "    j3.append(ds.WindowWidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ji=[]\n",
    "for i in range(len(j1)):\n",
    "    ji.append(j1[i]*j2[i])\n",
    "print(ji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(ji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfEpochs=15\n",
    "for type in ['flip','rotate90','blur']:\n",
    "    for k in range(5):\n",
    "        x_train,y_train,x_test,y_test,trainIndexes,testIndexes,testCases=divideIntoSets(negSlices,rpeSlices,rpeIndexes,caseNames0,caseNames1,k)\n",
    "        x_train,y_train=augment(x_train,y_train,type)\n",
    "        for j in range(6):\n",
    "            startTime=perf_counter()\n",
    "            predictions,trainPreds=predict(x_train,y_train,x_test,numberOfEpochs)\n",
    "            print(evaluatePreds(predictions,y_test,trainPreds,y_train))\n",
    "            np.savetxt('rpe_unet_{}_{}_{}_trainPreds.txt'.format(type,k,j),trainPreds)\n",
    "            np.savetxt('rpe_unet_{}_{}_{}_predictions.txt'.format(type,k,j),predictions)\n",
    "            endTime=perf_counter()\n",
    "            processingTime=endTime-startTime\n",
    "            np.savetxt('rpe_unet_{}_{}_{}_time.txt'.format(type,k,j),np.array([k,j,processingTime]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "for k in range(5):\n",
    "    x_train,y_train,x_test,y_test,trainIndexes,testIndexes,testCases=divideIntoSets(negSlices,rpeSlices,rpeIndexes,caseNames0,caseNames1,k)\n",
    "    x_train,y_train=augment(x_train,y_train,'flip')\n",
    "    for j in range(6):\n",
    "        trainPreds=np.loadtxt('rpe_unet_flip_{}_{}_trainPreds.txt'.format(k,j))\n",
    "        predictions=np.loadtxt('rpe_unet_flip_{}_{}_predictions.txt'.format(k,j))\n",
    "        #y.append(evaluatePreds(predictions,y_test,trainPreds,y_train)[3])\n",
    "        time=np.loadtxt('rpe_unet_flip_{}_{}_time.txt'.format(k,j))[2]\n",
    "        y.append(time/60)\n",
    "y=np.array(y)\n",
    "print(y)\n",
    "\n",
    "type='blur'\n",
    "y1=[]\n",
    "for k in range(5):\n",
    "    x_train,y_train,x_test,y_test,trainIndexes,testIndexes,testCases=divideIntoSets(negSlices,rpeSlices,rpeIndexes,caseNames0,caseNames1,k)\n",
    "    x_train,y_train=augment(x_train,y_train,type)\n",
    "    for j in range(6):\n",
    "        trainPreds=np.loadtxt('rpe_unet_{}_{}_{}_trainPreds.txt'.format(type,k,j))\n",
    "        predictions=np.loadtxt('rpe_unet_{}_{}_{}_predictions.txt'.format(type,k,j))\n",
    "        #y1.append(evaluatePreds(predictions,y_test,trainPreds,y_train)[3])\n",
    "        time=np.loadtxt('rpe_unet_{}_{}_{}_time.txt'.format(type,k,j))[2]\n",
    "        y1.append(time/60)\n",
    "y1=np.array(y1)\n",
    "print(y1)\n",
    "\n",
    "print(np.mean(y))\n",
    "print(np.std(y))\n",
    "print(np.median(y))\n",
    "print(np.mean(y1))\n",
    "print(np.std(y1))\n",
    "print(np.median(y1))\n",
    "print(wilcoxon(y,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "j=0\n",
    "x_train,y_train,x_test,y_test,trainIndexes,testIndexes,testCases=divideIntoSets(negSlices,rpeSlices,rpeIndexes,caseNames0,caseNames1,k)\n",
    "x_train,y_train=augment(x_train,y_train,'flip')\n",
    "trainPreds=np.loadtxt('rpe_unet_flip_{}_{}_trainPreds.txt'.format(k,j))\n",
    "predictions=np.loadtxt('rpe_unet_flip_{}_{}_predictions.txt'.format(k,j))\n",
    "print(evaluatePreds(predictions,y_test,trainPreds,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors=convertIntoVectors(y_train,y_test,trainIndexes,testIndexes,trainPreds,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluateVectors(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors,train_real_lengths,test_real_lengths=vectorsToSameDimension(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_vectors.shape)\n",
    "print(y_train_vectors.shape)\n",
    "print(x_test_vectors.shape)\n",
    "print(y_test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes=[]\n",
    "test_indexes=[]\n",
    "for i in range(len(y_train_vectors)):\n",
    "    train_indexes.append(np.max(y_train_vectors[i]))\n",
    "for i in range(len(y_test_vectors)):\n",
    "    test_indexes.append(np.max(y_test_vectors[i]))\n",
    "train_indexes=np.array(train_indexes)\n",
    "test_indexes=np.array(test_indexes)\n",
    "print(train_indexes.shape)\n",
    "print(test_indexes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length=12\n",
    "\n",
    "model = keras.Sequential([keras.layers.Embedding(12,embedding_vector_length),\n",
    "                        keras.layers.LSTM(10),\n",
    "                        keras.layers.Dense(1, activation='sigmoid')\n",
    "]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "numberOfEpochs=30\n",
    "history=model.fit(x=x_train_vectors,y=train_indexes,epochs=numberOfEpochs,validation_split=0.3,shuffle=True)\n",
    "predicted_indexes=model.predict(x_test_vectors)\n",
    "trainPred_indexes=model.predict(x_train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier()\n",
    "rf.fit(x_train_vectors,train_indexes)\n",
    "predicted_indexes=rf.predict(x_test_vectors)\n",
    "trainPred_indexes=rf.predict(x_train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1=svm.SVC()\n",
    "svm1.fit(x_train_vectors,train_indexes)\n",
    "predicted_indexes=svm1.predict(x_test_vectors)\n",
    "trainPred_indexes=svm1.predict(x_train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluatePreds(predicted_indexes,test_indexes,trainPred_indexes,train_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "y1=[]\n",
    "for k in range(5):\n",
    "    x_train,y_train,x_test,y_test,trainIndexes,testIndexes,testCases=divideIntoSets(negSlices,rpeSlices,rpeIndexes,caseNames0,caseNames1,k)\n",
    "    for j in range(6):\n",
    "        trainPreds=np.loadtxt('rpe_unet_flip_{}_{}_trainPreds.txt'.format(k,j))\n",
    "        predictions=np.loadtxt('rpe_unet_flip_{}_{}_predictions.txt'.format(k,j))\n",
    "        x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors=convertIntoVectors(y_train,y_test,trainIndexes,testIndexes,trainPreds,predictions)\n",
    "        y.append(evaluateVectors(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors)[3])\n",
    "        x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors,train_real_lengths,test_real_lengths=vectorsToSameDimension(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors)\n",
    "        train_indexes=[]\n",
    "        test_indexes=[]\n",
    "        for l in range(len(y_train_vectors)):\n",
    "            train_indexes.append(np.max(y_train_vectors[l]))\n",
    "        for l in range(len(y_test_vectors)):\n",
    "            test_indexes.append(np.max(y_test_vectors[l]))\n",
    "        train_indexes=np.array(train_indexes)\n",
    "        test_indexes=np.array(test_indexes)\n",
    "        rf=RandomForestClassifier()\n",
    "        rf.fit(x_train_vectors,train_indexes)\n",
    "        predicted_indexes=rf.predict(x_test_vectors)\n",
    "        trainPred_indexes=rf.predict(x_train_vectors)\n",
    "        #svm1=svm.SVC()\n",
    "        #svm1.fit(x_train_vectors,train_indexes)\n",
    "        #predicted_indexes=svm1.predict(x_test_vectors)\n",
    "        #trainPred_indexes=svm1.predict(x_train_vectors)\n",
    "        y1.append(evaluatePreds(predicted_indexes,test_indexes,trainPred_indexes,train_indexes)[3])\n",
    "y=np.array(y)\n",
    "print(y)\n",
    "y1=np.array(y1)\n",
    "print(y1)\n",
    "\n",
    "print(np.mean(y))\n",
    "print(np.std(y))\n",
    "print(np.median(y))\n",
    "print(np.mean(y1))\n",
    "print(np.std(y1))\n",
    "print(np.median(y1))\n",
    "print(wilcoxon(y,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "for k in range(5):\n",
    "    x_train,y_train,x_test,y_test,trainIndexes,testIndexes,testCases=divideIntoSets(negSlices,rpeSlices,rpeIndexes,caseNames0,caseNames1,k)\n",
    "    for j in range(6):\n",
    "        trainPreds=np.loadtxt('rpe_unet_flip_{}_{}_trainPreds.txt'.format(k,j))\n",
    "        predictions=np.loadtxt('rpe_unet_flip_{}_{}_predictions.txt'.format(k,j))\n",
    "        x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors=convertIntoVectors(y_train,y_test,trainIndexes,testIndexes,trainPreds,predictions)\n",
    "        y.append(evaluateVectors(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors)[7])\n",
    "y=np.array(y)\n",
    "print(y)\n",
    "\n",
    "print(np.mean(y))\n",
    "print(np.std(y))\n",
    "print(np.median(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=4\n",
    "x_train,y_train,x_test,y_test,trainIndexes,testIndexes,testCases=divideIntoSets(negSlices,rpeSlices,rpeIndexes,caseNames0,caseNames1,k)\n",
    "j=0\n",
    "trainPreds=np.loadtxt('rpe_unet_flip_{}_{}_trainPreds.txt'.format(k,j))\n",
    "predictions=np.loadtxt('rpe_unet_flip_{}_{}_predictions.txt'.format(k,j))\n",
    "x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors=convertIntoVectors(y_train,y_test,trainIndexes,testIndexes,trainPreds,predictions)\n",
    "TN_indexes1,TP_indexes1,FN_indexes1,FP_indexes1,FN_patients1,FP_patients1=listFinalIndexes(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors,testCases)\n",
    "for j in range(1,6):\n",
    "    trainPreds=np.loadtxt('rpe_unet_flip_{}_{}_trainPreds.txt'.format(k,j))\n",
    "    predictions=np.loadtxt('rpe_unet_flip_{}_{}_predictions.txt'.format(k,j))\n",
    "    x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors=convertIntoVectors(y_train,y_test,trainIndexes,testIndexes,trainPreds,predictions)\n",
    "    TN_indexes,TP_indexes,FN_indexes,FP_indexes,FN_patients,FP_patients=listFinalIndexes(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors,testCases)\n",
    "    TN_indexes1=intersection(TN_indexes1,TN_indexes)\n",
    "    TP_indexes1=intersection(TP_indexes1,TP_indexes)\n",
    "    FN_indexes1=intersection(FN_indexes1,FN_indexes)\n",
    "    FP_indexes1=intersection(FP_indexes1,FP_indexes)\n",
    "    FN_patients1=intersection(FN_patients1,FN_patients)\n",
    "    FP_patients1=intersection(FP_patients1,FP_patients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FN_patients1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FP_patients1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TN_indexes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=[]\n",
    "x_vectors=[]\n",
    "for i in range(len(testIndexes)):\n",
    "    vector.append(x_test[i])\n",
    "    if i==len(testIndexes)-1:\n",
    "        x_vectors.append(vector)\n",
    "    else:\n",
    "        if testIndexes[i+1]==0:\n",
    "            x_vectors.append(vector)\n",
    "            vector=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=np.ones((260,260))\n",
    "img[0:128,0:128]=x_vectors[1][15]\n",
    "img[0:128,132:260]=x_vectors[7][21]\n",
    "img[132:260,0:128]=x_vectors[3][13]\n",
    "img[132:260,132:260]=x_vectors[13][10]\n",
    "plt.imshow(img,cmap='gray')\n",
    "fig=plt.gcf()\n",
    "plt.axis('off')\n",
    "#fig.savefig('fig_TN',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_test[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kuv=np.ones((768,128))\n",
    "for i in range(512):\n",
    "    for j in range(128):\n",
    "        kuv[767-i-2*j,j]=x_test[0][127-int(i/4),j]\n",
    "plt.imshow(kuv,cmap='gray')\n",
    "fig=plt.gcf()\n",
    "plt.axis('off')\n",
    "fig.savefig('fig1',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test_vectors[0][0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kuv=np.ones((768,768))\n",
    "for u in range(12):\n",
    "    u1=[540,510,480,450,420,390,360,120,90,60,30,0][11-u]\n",
    "    u0=39-u\n",
    "    if u>4:\n",
    "        u0=11-u\n",
    "    for i in range(128):\n",
    "        for j in range(512):\n",
    "            kuv[767-i-u1,j+i]=x_test[u0][127-i,int(j/4)]\n",
    "plt.imshow(kuv,cmap='gray')\n",
    "fig=plt.gcf()\n",
    "plt.axis('off')\n",
    "#fig.savefig('figl',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TN_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprList=[]\n",
    "fprList=[]\n",
    "\n",
    "for k in range(5):\n",
    "    x_train,y_train,x_test,y_test,trainIndexes,testIndexes,testCases=divideIntoSets(negSlices,rpeSlices,rpeIndexes,caseNames0,caseNames1,k)\n",
    "    for j in range(6):\n",
    "        trainPreds=np.loadtxt('rpe_unet_flip_{}_{}_trainPreds.txt'.format(k,j))\n",
    "        predictions=np.loadtxt('rpe_unet_flip_{}_{}_predictions.txt'.format(k,j))\n",
    "        x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors=convertIntoVectors(y_train,y_test,trainIndexes,testIndexes,trainPreds,predictions)\n",
    "        tpr,fpr,tpr1,fpr1=infoForROC(x_train_vectors,y_train_vectors,x_test_vectors,y_test_vectors)\n",
    "        tprList.append(tpr1)\n",
    "        fprList.append(fpr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr=0.001*np.array(range(1000))\n",
    "tprArray=np.zeros((30,1000))\n",
    "for i in range(30):\n",
    "    for j in range(1000):\n",
    "        tprArray[i,j]=np.min(tprList[i][fprList[i]>0.001*j])\n",
    "tpr=np.median(tprArray,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr,tpr,color='blue')\n",
    "plt.plot([0,1],[0,1],'--',color='lightgray')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve on patient level')\n",
    "fig=plt.gcf()\n",
    "#fig.savefig('figr1',bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
